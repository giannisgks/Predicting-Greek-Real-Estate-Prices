{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3796087,"sourceType":"datasetVersion","datasetId":2263988}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Introduction\n\nThe dataset used in this analysis is provided by **Argyris Anastasolpoulos**.  \nIt contains listings of residential properties in Greece for the year 2022, covering the regions of **Attiki** and **Thessaloniki**.\n\n**Dataset**: [Greece Property Listings](https://www.kaggle.com/datasets/argyrisanastopoulos/greece-property-listings)  \n**Source**: Kaggle  \n","metadata":{}},{"cell_type":"markdown","source":"## 2. Data Loading\nLoad the dataset and preview the structure of the data.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import StrMethodFormatter\n\nimport pandas as pd\nimport numpy as np\n\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    explained_variance_score,\n    median_absolute_error,\n    mean_squared_error,\n    r2_score\n)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\ndf = pd.read_csv(\"/kaggle/input/greece-property-listings/greece_listings.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Original Data Shape**","metadata":{}},{"cell_type":"code","source":"df\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape\ndf.info()\ndf.head()\ndf.describe()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"**We remove the houses located outside Attica, specifically in Thessaloniki, since there are only 1,187 compared to 18,813 in Attica.**","metadata":{}},{"cell_type":"code","source":"attica_count = df[df[\"location_region\"] == \"Αττική\"].shape[0]\nprint(f\"Number of properties in Αττική: {attica_count}\")\ndf = df[df[\"location_region\"] == \"Αττική\"].reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We print the distinct regions of Attica; later, we will create separate fields for each region.**","metadata":{}},{"cell_type":"code","source":"unique_locations = df[\"location_name\"].unique()\nprint(f\"Number of unique locations: {len(unique_locations)}\")\ndf[\"location_name\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We keep the regions with more than 100 houses, so the model can learn consistent patterns without overfitting to areas with limited data.**","metadata":{}},{"cell_type":"code","source":"loc_df = df[\"location_name\"].value_counts().reset_index()\nloc_df.columns = [\"location_name\", \"count\"]\nlocations_to_keep = loc_df[loc_df[\"count\"] >= 100][\"location_name\"]\ndf = df[df[\"location_name\"].isin(locations_to_keep)].reset_index(drop=True)\n\nunique_locations = df[\"location_name\"].unique()\nprint(f\"Number of unique locations with more than 100 houses: {len(unique_locations)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We convert the region names into Latin characters in order to create new separate fields for each region. This process is called one-hot encoding and transforms an alphanumeric feature into multiple boolean features. These new features are much easier for the model to handle.**","metadata":{}},{"cell_type":"code","source":"greek_to_english_map = {\n    \"Αθήνα\": \"athina\",\n    \"Αχαρνές\": \"acharnes\",\n    \"Καλλιθέα\": \"kallithea\",\n    \"Βάρη - Βούλα - Βουλιαγμένη\": \"vari-voula-vouliagmeni\",\n    \"Πειραιάς\": \"peiraias\",\n    \"Χαλάνδρι\": \"chalandri\",\n    \"Γλυφάδα\": \"glyfada\",\n    \"Άλιμος\": \"alimos\",\n    \"Μαρούσι\": \"marousi\",\n    \"Νίκαια\": \"nikaia\",\n    \"Διόνυσος\": \"dionysos\",\n    \"Ίλιον\": \"ilion\",\n    \"Παλαιό Φάληρο\": \"palaio-faliro\",\n    \"Νέα Σμύρνη\": \"nea-smyrni\",\n    \"Ζωγράφου\": \"zografou\",\n    \"Ηλιούπολη\": \"ilioupoli\",\n    \"Βύρωνας\": \"vyronas\",\n    \"Νέα Ιωνία\": \"nea-ionia\",\n    \"Γαλάτσι\": \"galatsi\",\n    \"Περιστέρι\": \"peristeri\",\n    \"Αργυρούπολη\": \"argyroupoli\",\n    \"Λαυρεωτική\": \"lavreotiki\",\n    \"Κρωπία\": \"kropia\",\n    \"Ελληνικό - Αργυρούπολη\": \"elliniko-argyroupoli\",\n    \"Μοσχάτο - Ταύρος\": \"moschato-tavros\",\n    \"Παγκράτι\": \"pagkrati\",\n    \"Άγιος Δημήτριος\": \"agios-dimitrios\",\n    \"Αιγάλεω\": \"aigaleo\",\n    \"Άγιοι Ανάργυροι\": \"agioi-anargyroi\",\n    \"Δάφνη\": \"dafni\",\n    \"Ραφήνα Πικέρμι\": \"rafina-pikermi\",\n    \"Σπάτα\": \"spata\",\n    \"Κερατσίνι-Δραπετσώνα\": \"keratsini-drapetsona\",\n    \"Χολαργός\": \"cholargos\",\n    \"Φιλοθέη - Ψυχικό\": \"filothei-psychiko\"\n}\ndf[\"location_name_eng\"] = df[\"location_name\"].map(greek_to_english_map)\ndf[\"location_name_eng\"] = df[\"location_name_eng\"].fillna(\"unknown\")\n\nlocation_name_eng = df[\"location_name_eng\"].unique()\n\nfor loc in location_name_eng:\n    df[loc] = 0\n\nfor loc in location_name_eng:\n    df.loc[df[\"location_name_eng\"] == loc, loc] = 1\n\nregion_counts = {}\n\nfor loc in location_name_eng:\n    count_ones = df[loc].sum()\n    region_counts[loc] = count_ones\n    print(f\"Region '{loc}': {count_ones} houses\")\n\ntotal_ones = sum(region_counts.values())\ntotal_rows = df.shape[0]\n\nprint(f\"\\nTotal houses counted in one-hot columns: {total_ones}\")\nprint(f\"Total rows in dataset: {total_rows}\")\n\nassert total_ones == total_rows, \"Mismatch between one-hot counts and total rows!\"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The conversion was successful, without errors (for example, a house mistakenly belonging to 2 areas).\nThe next step is to delete the fields related to the name of the area that are no longer useful, so that the dataset is clean and easily readable.**","metadata":{}},{"cell_type":"markdown","source":"**Let’s check for res_address if there are specific areas or neighborhoods with more than 100 houses that could be made into separate fields and are not located in the center of Athens.**","metadata":{}},{"cell_type":"code","source":"res_address_counts = df[\"res_address\"].value_counts()\nres_address_over_100 = res_address_counts[res_address_counts > 100]\n\nunique_locations = df[\"location_name\"].unique()\n\nmissing_parts_counts = {}\n\nfor address in res_address_over_100.index:\n    parts = [p.strip() for p in address.split(\",\")]\n    \n    mask = (df[\"res_address\"] == address) & (df[\"location_name\"] != \"Αθήνα\")\n    count_filtered = mask.sum()\n    \n    for part in parts:\n        if part not in unique_locations and count_filtered > 0:\n            missing_parts_counts[part] = missing_parts_counts.get(part, 0) + count_filtered\n\nprint(\"Regions not in location_name that dont have location_name='Αθήνα':\")\nfor part, count in missing_parts_counts.items():\n    print(f\"{part}: {count}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Separate areas: Ekali, Vrilissia, Vari, Varkiza, Drosia, Kaminia (relatively larger or independent administrative areas).\nWe create new fields for these areas and make sure to remove them from the broader area they have belonged to so far. For example, houses in Ekali and Drosia belong to the area of Dionysos. To avoid a house belonging to two areas, we will remove the '1' from the Dionysos area.**","metadata":{}},{"cell_type":"markdown","source":"**Also, there is the case where the name of one area appears together with the name of another in the res_address field.\nAs shown below, the alphanumeric string \"Vari, Varkiza\" appears, which can cause confusion because we will create a separate field for Vari and a separate one for Varkiza. In such a case, we will change the string to \"Varkiza.\"\nThe same happens with \"Piraeus, Kaminia.\"**","metadata":{}},{"cell_type":"code","source":"df[\"res_address\"] = df[\"res_address\"].str.replace(\"Βάρη,Βάρκιζα\", \"Βάρκιζα\", regex=False)\ndf[\"res_address\"] = df[\"res_address\"].str.replace(\"Πειραιάς,Καμίνια\", \"Καμίνια\", regex=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"greek_areas = [\"Εκάλη\", \"Βριλήσσια\", \"Βάρη\", \"Βάρκιζα\", \"Δροσιά\", \"Καμίνια\"]\n\ndef extract_main_area(address):\n    for area in greek_areas:\n        if area in address:\n            return area\n    return address\n\ndf[\"res_address\"] = df[\"res_address\"].apply(lambda x: extract_main_area(x) if pd.notnull(x) else x)\n\nmask_after = df[\"res_address\"].dropna().apply(lambda x: any(area in x for area in greek_areas))\n\nunique_cleaned = df.loc[mask_after, \"res_address\"].unique()\n\nfor addr in sorted(unique_cleaned):\n    print(addr)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We create a temporary field '6new_temp' that stores the area extracted from the res_address field.**","metadata":{}},{"cell_type":"code","source":"area_map = {\n    \"Εκάλη\": \"ekali\",\n    \"Βριλήσσια\": \"vrilissia\",\n    \"Βάρη\": \"vari\",\n    \"Βάρκιζα\": \"varkiza\",\n    \"Δροσιά\": \"drosia\",\n    \"Καμίνια\": \"kaminia\"\n}\n\ndef assign_area(address):\n    if pd.isna(address):\n        return ''\n    for greek_name, eng_name in area_map.items():\n        if greek_name in address:\n            return eng_name\n    return ''\n\ndf['6new_temp'] = df['res_address'].apply(assign_area)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We make sure to remove the 1 from the general area to which the house belonged.**","metadata":{}},{"cell_type":"code","source":"df.loc[df[\"6new_temp\"] == \"vari\", \"vari-voula-vouliagmeni\"] = 0\ndf.loc[df[\"6new_temp\"] == \"vari\", \"kropia\"] = 0\ndf.loc[df[\"6new_temp\"] == \"vari\", \"vari\"] = 1\n\ndf.loc[df[\"6new_temp\"] == \"varkiza\", \"vari-voula-vouliagmeni\"] = 0\ndf.loc[df[\"6new_temp\"] == \"varkiza\", \"kropia\"] = 0\ndf.loc[df[\"6new_temp\"] == \"varkiza\", \"varkiza\"] = 1\n\ndf.loc[df[\"6new_temp\"] == \"ekali\", \"dionysos\"] = 0\ndf.loc[df[\"6new_temp\"] == \"ekali\", \"acharnes\"] = 0\ndf.loc[df[\"6new_temp\"] == \"ekali\", \"ekali\"] = 1\n\ndf.loc[df[\"6new_temp\"] == \"vrilissia\", \"chalandri\"] = 0\ndf.loc[df[\"6new_temp\"] == \"vrilissia\", \"marousi\"] = 0\ndf.loc[df[\"6new_temp\"] == \"vrilissia\", \"vrilissia\"] = 1\n\ndf.loc[df[\"6new_temp\"] == \"drosia\", \"dionysos\"] = 0\ndf.loc[df[\"6new_temp\"] == \"drosia\", \"drosia\"] = 1\n\ndf.loc[df[\"6new_temp\"] == \"kaminia\", \"peiraias\"] = 0\ndf.loc[df[\"6new_temp\"] == \"kaminia\", \"kaminia\"] = 1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_columns = ['ekali', 'vrilissia', 'vari', 'varkiza', 'drosia', 'kaminia']\n\ndf[target_columns] = df[target_columns].fillna(0).astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We check if there is any ‘double’ area anywhere.**","metadata":{}},{"cell_type":"code","source":"regions = ['palaio-faliro', 'acharnes', 'pagkrati', 'athina', 'ilion', 'galatsi', 'dafni', 'kallithea',\n           'argyroupoli', 'peristeri', 'kropia', 'nea-ionia', 'moschato-tavros', 'vari-voula-vouliagmeni',\n           'nikaia', 'peiraias', 'zografou', 'chalandri', 'ilioupoli', 'dionysos', 'alimos', 'marousi',\n           'nea-smyrni', 'aigaleo', 'glyfada', 'spata', 'keratsini-drapetsona', 'agioi-anargyroi',\n           'agios-dimitrios', 'rafina-pikermi', 'lavreotiki', 'elliniko-argyroupoli', 'vyronas', 'cholargos',\n           'filothei-psychiko', 'ekali', 'vrilissia', 'vari', 'varkiza', 'drosia', 'kaminia']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_ones = df[regions].sum().sum()\nprint(f\"Total 1 in all areas: {total_ones}\")\nprint(f\"Rows in the dataset: {len(df)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The fields that describe the area with alphanumeric strings are no longer needed, since we successfully converted the areas using one-hot encoding.**","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"location_name\"], inplace=True)\ndf.drop(columns=[\"location_name_eng\"], inplace=True)\ndf.drop(columns=[\"location_region\"], inplace=True)\ndf.drop(columns=[\"res_address\"], inplace=True)\ndf.drop(columns=[\"6new_temp\"], inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The creation date of the listing and whether it has been deleted or not and when is also not useful**","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"res_date\"], inplace=True)\ndf.drop(columns=[\"deleted\"], inplace=True)\ndf.drop(columns=[\"deleted_at\"], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Below we see the distribution of the \"res_type\" field which shows the type of property being sold**","metadata":{}},{"cell_type":"code","source":"res_type_counts = df['res_type'].value_counts().reset_index()\nres_type_counts.columns = ['res_type', 'count']\n\nplt.figure(figsize=(10, 6))\nplt.figure(figsize=(10, 6))\nax = sns.barplot(\n    data=res_type_counts,\n    x='res_type',\n    y='count',\n    palette='viridis'\n)\n\n\nplt.title('Distribution of res_type')\nplt.xlabel('res_type')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\n\nfor i, row in res_type_counts.iterrows():\n    ax.text(i, row['count'] + 0.5, str(row['count']), ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The types \"Building\" and \"House\" make up only 4.6% of the \"res_type\" field. These categories have much lower frequency compared to the others and relatively vague characteristics, making reliable analysis or modeling difficult. For this reason, they were removed from the dataset.**","metadata":{}},{"cell_type":"code","source":"df = df[~df['res_type'].isin(['Κτίριο', 'Οικία'])]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The field construction_year indicates the year the property was built. For a more useful representation, it will be converted to house_age, meaning the age of the property in years (current year - construction year). This variable is likely to be more relevant to the price, as newer or older properties may exhibit different patterns in terms of their value.**","metadata":{}},{"cell_type":"code","source":"df['house_age'] = (2022 - df['construction_year']).astype('Int64')\ndf.drop(columns=[\"construction_year\"], inplace=True)\ncounts = df['house_age'].value_counts().sort_index()\n\nprint(\"Discrete values of house_age:\")\nprint(counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Checking fields for missing values**","metadata":{}},{"cell_type":"code","source":"print(df.isna().sum().head(19))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The parking field is missing 78% of its values, so it will undoubtedly be removed.**","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"parking\"], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**In the bathrooms field, before handling the NaN values, we will perform an analysis to observe the variation of the values in the bathrooms field.**","metadata":{}},{"cell_type":"code","source":"pd.options.display.float_format = '{:,.2f}'.format\n\nsummary = df.groupby('bathrooms')['res_price'].agg(['mean', 'count']).sort_index()\n\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**As we can observe, properties with 6 or more bathrooms are found in fewer than 100 homes, so we will remove these entries to avoid overfitting.**","metadata":{}},{"cell_type":"code","source":"df['bathrooms'] = df['bathrooms'].fillna(0).astype(int)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[df['bathrooms'] < 6]\n\nsummary = df.groupby('bathrooms')['res_price'].agg(['mean', 'count']).sort_index()\nprint(summary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**To fill in the NaN values (which were set to 0 for easier handling, since there were no houses with bathrooms = 0) in the bathrooms column, we will use the property price (res_price) and categorize each property based on the closest average price per number of bathrooms. For example, if a property with a NaN in the bathrooms column is priced at €790,000, we will assign it to the category with 3 bathrooms if that has the closest average price. This method is more reasonable than simply filling in the overall average number of bathrooms, which would lead to many incorrect entries.**","metadata":{}},{"cell_type":"code","source":"df = df.copy()\n\nmean_prices = df[df['bathrooms'] != 0].groupby('bathrooms')['res_price'].mean().sort_index()\n\nbins = [-float('inf')]\nlabels = []\n\nfor i in range(len(mean_prices) - 1):\n    midpoint = (mean_prices.iloc[i] + mean_prices.iloc[i + 1]) / 2\n    bins.append(midpoint)\n    labels.append(int(mean_prices.index[i]))\n\nlabels.append(int(mean_prices.index[-1]))\nbins.append(float('inf'))\n\ndf['temp_bathroom'] = pd.cut(df['res_price'], bins=bins, labels=labels, right=False)\n\ndf.loc[df['bathrooms'] == 0, 'bathrooms'] = df.loc[df['bathrooms'] == 0, 'temp_bathroom']\n\ndf.drop(columns='temp_bathroom', inplace=True)\ndf['bathrooms'] = df['bathrooms'].astype(int)\n\nprint(\"Houses per number of bathrooms:\")\nprint(df['bathrooms'].value_counts().sort_index())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We observe that the total number of houses in each category has increased. We follow the exact same logic for the bedrooms as well.**","metadata":{}},{"cell_type":"code","source":"df['bedrooms'] = df['bedrooms'].fillna(0).astype(int)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.options.display.float_format = '{:,.2f}'.format\n\nsummary = df.groupby('bedrooms')['res_price'].agg(['mean', 'count']).sort_index()\n\nprint(summary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Let’s not forget that the 0 bedrooms value is temporary and represents the NaN values!**","metadata":{}},{"cell_type":"code","source":"df = df[df['bedrooms'] < 7]\n\nsummary = df.groupby('bedrooms')['res_price'].agg(['mean', 'count']).sort_index()\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.copy()\n\nmean_prices = df[df['bedrooms'] != 0].groupby('bedrooms')['res_price'].mean().sort_index()\n\nbins = [-float('inf')]\nlabels = []\n\nfor i in range(len(mean_prices) - 1):\n    midpoint = (mean_prices.iloc[i] + mean_prices.iloc[i + 1]) / 2\n    bins.append(midpoint)\n    labels.append(int(mean_prices.index[i]))\n\nlabels.append(int(mean_prices.index[-1]))\nbins.append(float('inf'))\n\ndf['temp_bedroom'] = pd.cut(df['res_price'], bins=bins, labels=labels, right=False)\n\ndf.loc[df['bedrooms'] == 0, 'bedrooms'] = df.loc[df['bedrooms'] == 0, 'temp_bedroom']\n\ndf.drop(columns='temp_bedroom', inplace=True)\ndf['bedrooms'] = df['bedrooms'].astype(int)\n\nprint(\"Number of houses per number of bedrooms\")\nprint(df['bedrooms'].value_counts().sort_index())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**For the fields that have very few missing values, such as 'res_price_sqr' and 'levels', with 1 and 6 missing entries respectively, we will simply remove those entries.**","metadata":{}},{"cell_type":"code","source":"df = df.dropna(subset=['res_price_sqr'])\ndf = df.dropna(subset=['levels'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.isna().sum().head(19))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Now, only the status and energyclass fields remain. Let's visualize how many values each category has in these two features.**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_value_counts(column):\n    counts = df[column].value_counts(dropna=False)\n    plt.figure(figsize=(10,6))\n    ax = sns.barplot(x=counts.index.astype(str), y=counts.values)\n    plt.title(f'Distribution for {column}')\n    plt.ylabel('Total')\n    plt.xlabel(column)\n    plt.xticks(rotation=45, ha='right')\n    \n    for i, v in enumerate(counts.values):\n        ax.text(i, v + max(counts.values)*0.01, str(v), ha='center', fontsize=10)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_value_counts('status')\n\nplot_value_counts('energyclass')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**For the status field, it is observed that there is a value \"Άλλη Κατάσταση\" (\"Other Status\") indicating that the property's status is not available. The nan values, which represent missing entries, follow a similar logic. Therefore, \"Άλλη Κατάσταση\" will be renamed to \"Άγνωστη Κατάσταση\" (\"Unknown Status\") and will also include the nan values.**","metadata":{}},{"cell_type":"code","source":"df['status'] = df['status'].replace('Άλλη κατάσταση', 'Άγνωστη Κατάσταση')\ndf['status'] = df['status'].fillna('Άγνωστη Κατάσταση')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**For the energyclass field, along the same lines, the nan values will be merged with the entries labeled \"Εκρεμμεί\" (\"Pending\"), which indicate unknown energy status, just like the nan values. This combined category will be renamed to \"Άγνωστη Κλάση\" (\"Unknown Class\"). Additionally, the entries with the value \"Εξαιρείται\" (\"Excluded\") will be removed since they only account for 44 cases.**","metadata":{}},{"cell_type":"code","source":"df['energyclass'] = df['energyclass'].replace('Εκρεμμεί', 'Άγνωστη Κλάση')\ndf['energyclass'] = df['energyclass'].fillna('Άγνωστη Κλάση')\n\ndf = df[df['energyclass'] != 'Εξαιρείται']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**For both status and energyclass, we will apply one-hot encoding so that the information is represented in boolean form instead of as text labels.**","metadata":{}},{"cell_type":"code","source":"print(sorted(df['energyclass'].dropna().unique()))\nprint(sorted(df['status'].dropna().unique()))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"energyclass_map = {\n    'Α+': ['a+_class'],\n    'Α': ['a_class'],\n    'Β+': ['b+_class'],\n    'Β': ['b_class'],\n    'Γ': ['c_class'],\n    'Δ': ['d_class'],\n    'Ε': ['e_class'],\n    'Ζ': ['z_class'],\n    'Η': ['h_class'],\n    'Μη αποδοτικό': ['non_efficient_class'],\n    'Άγνωστη Κλάση': ['class_unknown_class']\n}\n\nfor col in set(sum(energyclass_map.values(), [])):\n    df[col] = 0\n    df[col] = df[col].astype(int)\n\ndef assign_energyclass_fields(row):\n    for key in energyclass_map.get(row['energyclass'], []):\n        row[key] = 1\n    return row\n\ndf = df.apply(assign_energyclass_fields, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"status_map = {\n    'Άριστη': ['excellent'],\n    'Καλή': ['good'],\n    'Ανακαινισμένο': ['renovated'],\n    'Νεόδμητο': ['brand_new'],\n    'Υπο κατασκευή': ['under_construction'],\n    'Χρήζει ανακαίνισης': ['needs_renovation'],\n    'Ημιτελές': ['unfinished'],\n    'Άγνωστη Κατάσταση': ['unknown_status']\n}\n\nfor col in set(sum(status_map.values(), [])):\n    df[col] = 0\n    df[col] = df[col].astype(int)\n\ndef assign_status_fields(row):\n    for key in status_map.get(row['status'], []):\n        row[key] = 1\n    return row\n\ndf = df.apply(assign_status_fields, axis=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We check if the new fields were created successfully, and then we remove the original ones.**","metadata":{}},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=[\"status\"], inplace=True)\ndf.drop(columns=[\"energyclass\"], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Checking the levels field, we observe that many entries refer to properties spanning more than one floor. Additionally, there are many different floor combinations. To reduce complexity, we will keep only the values that appear in more than 100 entries, just as we did for the areas field.**","metadata":{}},{"cell_type":"code","source":"value_counts = df['levels'].value_counts()\n\nvalid_levels = value_counts[value_counts > 100].index\n\ndf_filtered = df[df['levels'].isin(valid_levels)].copy()\n\nprint(f\"Total before: {len(df)}\")\nprint(f\"Total after: {len(df_filtered)}\")\nprint(\"Discrete values left:\", list(df_filtered['levels'].unique()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df_filtered\nprint(\"Distribution of discrete values left:\")\n\ndistribution = df['levels'].value_counts().sort_index()\n\nfor level, count in distribution.items():\n    print(f\"{level}: {count}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We will follow the same approach used for the areas, creating new boolean fields to categorize the floors on which the property is located. Each corresponding field will have a value of 1 if the property is on that floor, and 0 otherwise. If a property spans multiple floors, it will have 1s in multiple fields. This way, the model gains more detailed information.**","metadata":{}},{"cell_type":"code","source":"floor_map = {\n    '1ος': ['1_floor'],\n    '2ος': ['2_floor'],\n    '3ος': ['3_floor'],\n    '3ος,4ος': ['3_4_floor'],\n    '4ος': ['4_floor'],\n    '4ος,5ος': ['4_5_floor'],\n    '5ος': ['5_floor'],\n    '5ος,6ος': ['5_6_floor'],\n    '6ος': ['6_floor'],\n    '7ος': ['7_floor'],\n    'Ημιυπόγειο': ['semi_basement'],\n    'Ημιώροφος': ['semi_floor'],\n    'Ισόγειο': ['0_floor'],\n    'Ισόγειο,1ος': ['0_1_floor'],\n    'Ισόγειο,1ος,2ος': ['0_1_2_floor'],\n    'Υπερυψωμένο': ['elevated']\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for floor_level in set(sum(floor_map.values(), [])):\n    df[floor_level] = 0 \n    df[floor_level] = df[floor_level].astype(int) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_floors(row):\n    levels = row['levels']\n    floors = floor_map.get(levels, [])\n    for f in floors:\n        row[f] = 1\n    return row\n\ndf = df.apply(assign_floors, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Now we can remove the levels field that contained the floor information in string format.**","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"levels\"], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We observe that approximately 3.83% of the entries lack information about the property's age. Since age is an important factor affecting the price, it would not be appropriate to fill in these missing values with the average or any other simplistic method. Given that the percentage is relatively small, we choose to remove these entries from the dataset.**","metadata":{}},{"cell_type":"code","source":"total = len(df)\nna_count = df['house_age'].isna().sum()\nprint(f\"NaN values: {na_count} ({na_count / total:.2%} of total)\")\ndf = df[df['house_age'].notna()].copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**For the type of residence, represented by the res_type field, we will once again apply one-hot encoding.**","metadata":{}},{"cell_type":"code","source":"distribution = df['res_type'].value_counts().sort_index()\n\nfor level, count in distribution.items():\n    print(f\"{level}: {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type_map = {\n    'Διαμέρισμα': ['diamerisma'],\n    'Μεζονέτα': ['mezoneta'],\n    'Μονοκατοικία': ['monokatoikia']\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for types in set(sum(type_map.values(), [])):\n    df[types] = 0  # αρχικά 0\n    df[types] = df[types].astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_types(row):\n    types = row['res_type']\n    house_type = type_map.get(types, [])\n    for f in house_type:\n        row[f] = 1\n    return row\n\ndf = df.apply(assign_types, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=[\"res_type\"], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We've cleaned all the alphanumeric fields; now we'll perform a check for possible errors during the transformation—such as having a 1 in both the 'diamerisma' and 'mezoneta' fields simultaneously, which would be incorrect.**","metadata":{}},{"cell_type":"code","source":"res_type_columns = ['diamerisma', 'mezoneta', 'monokatoikia']\nstatus_columns = ['excellent', 'renovated', 'unfinished', 'good', 'brand_new', 'under_construction', 'needs_renovation', 'unknown_status']\nenergyclass_columns = ['a+_class', 'a_class', 'b_class', 'b+_class', 'c_class', 'd_class', 'e_class', 'z_class', 'h_class', 'non_efficient_class', 'class_unknown_class']\n\ndef check_single_ones(df, columns, group_name):\n    invalid = df[columns].sum(axis=1) != 1\n    count_invalid = invalid.sum()\n    print(f\"{group_name}: {count_invalid} lines with NOT exactly one '1'\")\n\ncheck_single_ones(df, res_type_columns, \"res_type\")\ncheck_single_ones(df, status_columns, \"status\")\ncheck_single_ones(df, energyclass_columns, \"energyclass\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The res_price_sqr field is removed because it gives the model direct access to the value it needs to predict, as it’s calculated by res_price_sqr = res_price / res_sqr. If res_price_sqr remains among the input features, the model essentially has the answer in a different form. This causes data leakage and overfitting—resulting in artificially high training performance but poor generalization on new data.**","metadata":{}},{"cell_type":"code","source":"y = df['res_price']\n\nX_with_leak = df.drop(columns=['res_price'])\n\nX_no_leak = df.drop(columns=['res_price', 'res_price_sqr'])\n\nmodel = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_estimators=50)\n\nr2_with_leak = cross_val_score(model, X_with_leak, y, scoring='r2', cv=5).mean()\nr2_no_leak = cross_val_score(model, X_no_leak, y, scoring='r2', cv=5).mean()\n\nprint(f\"R² no leakage: {r2_no_leak:.5f}\")\nprint(f\"R² with leakage (res_price_sqr): {r2_with_leak:.5f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=['res_price_sqr'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Feature Engineering\nThe next stage involves creating new features based either on existing data or external knowledge. This helps the model better \"understand\" the relationships between variables and train more effectively. To evaluate the contribution of a new feature, we will train the model with and without it and compare the results.**","metadata":{}},{"cell_type":"markdown","source":"**Lets use the correlation matrix to check what feautures correlate the most with price (res_price)**","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorr_matrix = df.corr()\ntarget_corr = corr_matrix['res_price'].sort_values(ascending=False)\n\nprint(target_corr.head(4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Let's test this idea with a new feature called total_rooms, which sums the number of bathrooms and bedrooms.**","metadata":{}},{"cell_type":"code","source":"df['total_rooms'] = df['bedrooms'] + df['bathrooms']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_with = df.drop(columns=['res_price'])\nX_without = X_with.drop(columns=['total_rooms'])\ny = df['res_price']\n\nmodel = xgb.XGBRegressor(objective='reg:squarederror', random_state=40)\n\nr2_with = cross_val_score(model, X_with, y, scoring='r2', cv=5).mean()\nr2_without = cross_val_score(model, X_without, y, scoring='r2', cv=5).mean()\n\nprint(f\"XGBoost R² with total_rooms: {r2_with:.4f}\")\nprint(f\"XGBoost R² no total_rooms: {r2_without:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**It’s rejected because, even slightly, it worsens the model’s prediction performance.**","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"total_rooms\"], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We will group the areas into 6 broader regions: North, South, East, West, Center, and Piraeus.**","metadata":{}},{"cell_type":"code","source":"regions_map = {\n    'voreia': [\n        'marousi', 'cholargos', 'filothei-psychiko', 'ekali', 'vrilissia', 'drosia', 'dionysos','nea-ionia'\n    ],\n    'notia': [\n        'glyfada', 'alimos', 'elliniko-argyroupoli','argyroupoli', 'vari-voula-vouliagmeni', 'vari','varkiza', 'lavreotiki','agios-dimitrios','ilioupoli','nea-smyrni','palaio-faliro','moschato-tavros'\n    ],\n    'anatolika': [\n        'spata', 'kropia', 'rafina-pikermi','acharnes'\n    ],\n    'dytika': [\n        'peristeri', 'aigaleo', 'agioi-anargyroi','ilion'\n    ],\n    'kentro': [\n        'athina', 'pagkrati','galatsi', 'dafni', 'kallithea', 'zografou', 'chalandri', 'vyronas'\n    ],\n    'pireas': [\n        'keratsini-drapetsona', 'argyroupoli', 'nikaia', 'peiraias','kaminia'\n    ]\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for group_name, areas in regions_map.items():\n    existing_areas = [area for area in areas if area in df.columns]\n    df[group_name] = df[existing_areas].sum(axis=1)\n\nprint(df[['voreia', 'notia', 'anatolika', 'dytika', 'kentro', 'pireas']].sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_with = df.drop(columns=['res_price'])\nX_without = X_with.drop(columns=['voreia', 'notia', 'anatolika', 'dytika', 'kentro', 'pireas'])\ny = df['res_price']\n\nmodel = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n\nr2_with = cross_val_score(model, X_with, y, scoring='r2', cv=5).mean()\nr2_without = cross_val_score(model, X_without, y, scoring='r2', cv=5).mean()\n\nprint(f\"XGBoost R² regions: {r2_with:.5f}\")\nprint(f\"XGBoost R² no regions: {r2_without:.5f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The room_density field combines the total number of rooms and divides it by the property's square meters. It represents the density of the house and, as we'll see later, significantly improves the model's performance.**","metadata":{}},{"cell_type":"code","source":"df['room_density'] = (df['bedrooms'] + df['bathrooms']) / df['res_sqr']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_with = df.drop(columns=['res_price'])\nX_without = X_with.drop(columns=['room_density'])\ny = df['res_price']\n\nmodel = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n\nr2_with = cross_val_score(model, X_with, y, scoring='r2', cv=5).mean()\nr2_without = cross_val_score(model, X_without, y, scoring='r2', cv=5).mean()\n\nprint(f\"XGBoost R² room_denstiy: {r2_with:.5f}\")\nprint(f\"XGBoost R² no room_density: {r2_without:.5f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=['room_density'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We introduce a new field, luxury_index, which indicates the amenities and features of the house.**","metadata":{}},{"cell_type":"code","source":"df['luxury_index'] = df['fireplace'] + df['cooling'] + df['safe_door'] + df['solar'] + df['excellent'] + df['brand_new']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_with = df.drop(columns=['res_price'])\nX_without = X_with.drop(columns=['luxury_index'])\ny = df['res_price']\n\nmodel = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n\nr2_with = cross_val_score(model, X_with, y, scoring='r2', cv=5).mean()\nr2_without = cross_val_score(model, X_without, y, scoring='r2', cv=5).mean()\n\nprint(f\"XGBoost R² luxury_index: {r2_with:.5f}\")\nprint(f\"XGBoost R² no luxury_index: {r2_without:.5f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['is_coastal'] = df['palaio-faliro'] + df['moschato-tavros'] + df['vari-voula-vouliagmeni'] + df['peiraias'] + df['alimos'] + df['glyfada'] + df['elliniko-argyroupoli'] + df['vari'] + df['varkiza'] + df['rafina-pikermi'] + df['lavreotiki']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_with = df.drop(columns=['res_price'])\nX_without = X_with.drop(columns=['is_coastal'])\ny = df['res_price']\n\nmodel = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n\nr2_with = cross_val_score(model, X_with, y, scoring='r2', cv=5).mean()\nr2_without = cross_val_score(model, X_without, y, scoring='r2', cv=5).mean()\n\nprint(f\"XGBoost R² is_coastal: {r2_with:.5f}\")\nprint(f\"XGBoost R² no is_coastal: {r2_without:.5f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=['is_coastal'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Outlier removal\n\nThe final stage before training the model is removing all outliers. At the end, we will compare the results before and after their removal.","metadata":{}},{"cell_type":"code","source":"df_original = df.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_house_age_with_counts(data, title):\n    plt.figure(figsize=(12, 6))\n    ax = sns.histplot(data['house_age'], bins=30, kde=False, color='skyblue')\n    plt.title(title)\n    plt.xlabel('House Age')\n    plt.ylabel('Count')\n    \n    for p in ax.patches:\n        height = p.get_height()\n        if height > 0:\n            ax.text(p.get_x() + p.get_width()/2, height + 3, int(height), ha='center', fontsize=9)\n    \n    plt.show()\n\nplot_house_age_with_counts(df, \"House Age Distribution BEFORE Removing Outliers\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[(df['house_age'] >= 0) & (df['house_age'] <= 70)].copy()\nprint(f\"New total nubmer of houses: {df.shape[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_price_distribution_with_stats(data, title):\n    plt.figure(figsize=(12, 6))\n    ax = sns.histplot(data['res_price'], bins=30, kde=False, color='coral')\n    plt.title(title)\n    plt.xlabel('Residential Price')\n    plt.ylabel('Count')\n\n    ax.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n\n    avg_price = data['res_price'].mean()\n    p95_price = data['res_price'].quantile(0.95)\n\n    ax.axvline(avg_price, color='blue', linestyle='--', linewidth=2, label=f'Average Price: {avg_price:,.0f}')\n    ax.axvline(p95_price, color='green', linestyle='--', linewidth=2, label=f'95th Percentile: {p95_price:,.0f}')\n\n    for p in ax.patches:\n        height = p.get_height()\n        if height > 0:\n            ax.text(p.get_x() + p.get_width()/2, height + 3, int(height), ha='center', fontsize=9)\n\n    plt.legend()\n    plt.show()\n\n# Call the function to plot\nplot_price_distribution_with_stats(df, \"Residential Price Distribution with Average and 95th Percentile\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"p95_price = df['res_price'].quantile(0.95)\n\nnum_above_p95 = df[df['res_price'] > p95_price].shape[0]\nprint(f\"Number of properties with price above the 95th percentile ({p95_price:,.0f}): {num_above_p95}\")\n\nnum_below_or_equal_p95 = df[df['res_price'] <= p95_price].shape[0]\nprint(f\"Number of properties with price below or equal to the 95th percentile ({p95_price:,.0f}): {num_below_or_equal_p95}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**As we observe from the chart, the number of properties priced above the 95th percentile (€891,200) is 740.\nThe number of properties priced at or below the 95th percentile (€891,200) is 14,051.\nThis means that 95% of the properties cost up to €891,200, while the remaining 5% are more expensive properties priced above this threshold. Obviously, we will remove these high-priced properties as outliers.**","metadata":{}},{"cell_type":"code","source":"df = df[df['res_price'] <= p95_price].copy()\nprint(f\"Νέος αριθμός ακινήτων: {df.shape[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Κρατάμε μόνο τα σπίτια με αξία μεγαλύτερη των 30000 ευρώ**","metadata":{}},{"cell_type":"code","source":"df = df[df['res_price'] >= 30000]\nprint(f\"New total nubmer of houses: {df.shape[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_res_sqr_distribution_10(data, title):\n    plt.figure(figsize=(16, 8))\n    \n    bins = np.arange(0, 1700 + 10, 10)\n    \n    ax = sns.histplot(data['res_sqr'], bins=bins, kde=False, color='mediumseagreen')\n    plt.title(title, fontsize=16)\n    plt.xlabel('Εμβαδόν (τ.μ.)', fontsize=14)\n    plt.ylabel('Αριθμός Ακινήτων', fontsize=14)\n    \n    plt.xlim(0, 1700)\n    ax.xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n    \n    for p in ax.patches:\n        if p.get_x() + p.get_width() <= 1700:\n            height = p.get_height()\n            if height > 0:\n                ax.text(p.get_x() + p.get_width()/2, height + 3, int(height), ha='center', fontsize=8, rotation=90)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_res_sqr_distribution_10(df, \"Res_sqr Distribution BEFORE Removing Outliers\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Properties with less than 20 sqm or more than 400 sqm will be removed as outliers.**","metadata":{}},{"cell_type":"code","source":"df = df[(df['res_sqr'] >= 20) & (df['res_sqr'] < 400)]\nprint(f\"New total nubmer of houses: {df.shape[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = ['res_price', 'res_sqr', 'house_age']\n\nprint(\"Comparison of Statistics Before and After Outlier Removal:\\n\")\n\nfor col in columns:\n    print(f\"Field: {col}\")\n    print(f\"  Before -> Size: {df_original.shape[0]:,}, Min: {df_original[col].min():,.2f}, Max: {df_original[col].max():,.2f}, Mean: {df_original[col].mean():,.2f}\")\n    print(f\"  After  -> Size: {df.shape[0]:,}, Min: {df[col].min():,.2f}, Max: {df[col].max():,.2f}, Mean: {df[col].mean():,.2f}\")\n    print()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Model Training\n    At first we use different Regression models like Ridge, Lasso, SGDRegressor etc.\n    The best result we get is R^2 = 0.74 with RMSE = 89450 From Ridge. We will try RandomForest and XGBoost in order to get improved perofrmance","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=['res_price'])\ny = df['res_price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Define models\nmodels = {\n    \"Random Forest\": RandomForestRegressor(\n        n_estimators=300,\n        max_depth=10,\n        random_state=42,\n        n_jobs=-1\n    ),\n    \"XGBoost\": XGBRegressor(\n        n_estimators=400,\n        learning_rate=0.05,\n        max_depth=6,\n        objective='reg:squarederror',\n        random_state=42,\n        n_jobs=-1\n    )\n}\n\n# Evaluate models\nresults = []\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n\n    results.append({\n        'Model': name,\n        'MAE': round(mae, 2),\n        'RMSE': round(rmse, 2),\n        'R²': round(r2, 4)\n    })\n\nresults_df = pd.DataFrame(results).sort_values(by='R²', ascending=False)\nprint(\"Performance Comparison:\")\nprint(results_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**As seen above XGBoost provides an improved result with R^2 = 0.79 and RMSE = 80106. Lets play with its parameters to get an even better result.**","metadata":{}},{"cell_type":"code","source":"best_params = {\n    'n_estimators': 800,\n    'learning_rate': 0.02,\n    'max_depth': 8,\n    'subsample': 0.85,\n    'colsample_bytree': 0.7,\n    'gamma': 0,\n    'min_child_weight': 1,\n    'objective': 'reg:squarederror',\n    'random_state': 42,\n    'n_jobs': -1\n}\n\n# Four variations around the best parameters\nparam_variations = [\n    {'learning_rate': 0.015, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.75},\n    {'learning_rate': 0.025, 'max_depth': 9, 'subsample': 0.9, 'colsample_bytree': 0.65},\n    {'learning_rate': 0.02, 'max_depth': 8, 'subsample': 0.85, 'colsample_bytree': 0.7}, \n    {'learning_rate': 0.03, 'max_depth': 8, 'subsample': 0.87, 'colsample_bytree': 0.72},\n]\n\nbest_rmse = float('inf')\nbest_r2 = float('inf')\nbest_config = None\n\nfor idx, variation in enumerate(param_variations, 1):\n    params = best_params.copy()\n    params.update(variation)\n\n    model = XGBRegressor(**params)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n    if rmse < best_rmse:\n        best_rmse = rmse\n        best_r2 = r2\n        best_config = variation\n\nprint(\"Best variation found:\")\nprint(best_config)\nprint(f\"RMSE: {best_rmse:.2f}\")\nprint(f\"R^2: {best_r2:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Conclusion\nBy carefully preprocessing and analyzing the available data, you can develop a predictive model that provides valuable insights. The key is to find the best fit between the model and the data to generate accurate predictions. Predicting house prices is inherently challenging, especially in today’s rapidly changing market. Nevertheless, building a reliable baseline model for home valuation is crucial, as it can potentially assist buyers, sellers, and real estate professionals in making more informed decisions.\n\nThank you for investing your time in this notebook. Your feedback is always welcome!\n\n**Ioannis Giakisikloglou**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}